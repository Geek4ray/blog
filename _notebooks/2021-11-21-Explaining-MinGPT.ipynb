{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-11-21-Explaining MinGPT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNmDvK9uXl9GQmvAGO++XXw"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ytIxWwZ2Q54"
      },
      "source": [
        "# Explaining MinGPT \n",
        "> This blog explains MinGPT Model implementation on Addition Data, by <a href=\"https://github.com/karpathy/minGPT\">Andrej Karpathy</a>.\n",
        "\n",
        "- toc: true \n",
        "- badges: true\n",
        "- comments: true\n",
        "- sticky_rank: 1\n",
        "- author: Rayan\n",
        "- image: images/diagram.png\n",
        "- categories: [MinGPT, transformers]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ5Rw95LEHSM"
      },
      "source": [
        "> `Objective : \"To Train a GPT model on a dedicated addition dataset to see if a Transformer can learn to add.\"`\n",
        "\n",
        "Our Objective is inspired by the addition section in the GPT-3 paper (Language Models are a few shot learners)- https://arxiv.org/pdf/2005.14165v4.pdf\n",
        "\n",
        "Code References Used :\n",
        "1. https://github.com/openai/gpt-2 - This has GPT code (only model code is taken but not training) in Tensorflow, which is converted to our usecase in PyTorch here.\n",
        "2. https://github.com/openai/image-gpt "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BBh6SucOrTW"
      },
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX27-hv_GrkK"
      },
      "source": [
        "# Imports\n",
        "import math\n",
        "import logging\n",
        "import gc\n",
        "import os\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data.dataloader import DataLoader"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwC71udQPnT8"
      },
      "source": [
        "## 2. Setting Our Seed :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2GY0RxFLVAt"
      },
      "source": [
        "# Seeding \n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Making deterministic, setting our seed\n",
        "set_seed(42)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbunfaORMXmZ"
      },
      "source": [
        "In order to generate data, we define our custom Addition Dataset Class. The sum of two n-digit numbers gives a third up to (n+1)-digit number. So our\n",
        "encoding will simply be the n-digit first number, n-digit second number, \n",
        "and (n+1)-digit result, all simply concatenated together. Because each addition\n",
        "problem is so structured, there is no need to bother the model with encoding\n",
        "+, =, or other tokens. Each possible sequence has the same length, and simply\n",
        "contains the raw digits of the addition problem.\n",
        "\n",
        "As a few examples, the 2-digit problems:\n",
        "- 85 + 50 = 135 becomes the sequence [8, 5, 5, 0, 1, 3, 5]\n",
        "- 6 + 39 = 45 becomes the sequence [0, 6, 3, 9, 0, 4, 5]\n",
        "etc.\n",
        "\n",
        "We will also only train GPT on the final (n+1)-digits because the first\n",
        "two n-digits are always assumed to be given. So when we give GPT an exam later,\n",
        "we will e.g. feed it the sequence [0, 6, 3, 9], which encodes that we'd like\n",
        "to add 6 + 39, and hope that the model completes the integer sequence with [0, 4, 5] in 3 sequential steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFqYoAHsNxGi"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class AdditionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Our Custom Dataset Class for Generating Data into Training and Test sets.\n",
        "\n",
        "    Returns addition problems of up to some number of digits in the inputs. We recall\n",
        "    that all GPT cares about are sequences of integers, and completing them according to\n",
        "    patterns in the data. Therefore, we have to somehow encode addition problems\n",
        "    as a sequence of integers.\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ndigit, split):\n",
        "        self.split = split # train/test\n",
        "        self.ndigit = ndigit\n",
        "        self.vocab_size = 10 # 10 possible digits 0..9\n",
        "        # +1 due to potential carry overflow, but then -1 because very last digit doesn't plug back\n",
        "        self.block_size = ndigit + ndigit + ndigit + 1 - 1\n",
        "        \n",
        "        # split up all addition problems into either training data or test data\n",
        "        num = (10**self.ndigit)**2 # total number of possible combinations\n",
        "        r = np.random.RandomState(1337) # make deterministic\n",
        "        perm = r.permutation(num)\n",
        "        num_test = min(int(num*0.2), 1000) # 20% of the whole dataset, or only up to 1000\n",
        "        self.ixes = perm[:num_test] if split == 'test' else perm[num_test:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.ixes.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # given a problem index idx, first recover the associated a + b\n",
        "        idx = self.ixes[idx]\n",
        "        nd = 10**self.ndigit\n",
        "        a = idx // nd\n",
        "        b = idx %  nd\n",
        "        c = a + b\n",
        "        render = f'%0{self.ndigit}d%0{self.ndigit}d%0{self.ndigit+1}d' % (a,b,c) # e.g. 03+25=28 becomes \"0325028\" \n",
        "        dix = [int(s) for s in render] # convert each character to its token index\n",
        "        # x will be input to GPT and y will be the associated expected outputs\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long) # predict the next token in the sequence\n",
        "        y[:self.ndigit*2-1] = -100 # we will only train in the output locations. -100 will mask loss to zero\n",
        "        return x, y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwWeLPKfN_nX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCuZ1LO-N_xF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffJ-kch0N_5N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opXsW_ZkOAB7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}