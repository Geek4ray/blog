{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "group_23_final.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPxN0bFvOEjiFZvIuMCuZrn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ytIxWwZ2Q54"
      },
      "source": [
        "# A Miniature Implementation of GPT Model\n",
        "> This blog explains a minimalistic implementation of GPT Model on the basis of an Addition Problem.\n",
        "\n",
        "- badges: true\n",
        "- comments: true\n",
        "- sticky_rank: 1\n",
        "- author: Group-23\n",
        "- image: images/diagram.png\n",
        "- categories: [GPT, transformers]\n",
        "\n",
        "Blog Link : https://geek4ray.github.io/blog/mingpt/transformers/2021/11/22/Implementing-Miniature-GPT-Model.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JemW4wPRzHJ5"
      },
      "source": [
        "This is Group-23 NLP Project : A Miniature Implemantation of GPT Model  \n",
        "\n",
        "Group Members :\n",
        "\n",
        "1. Mallikarjuna Naik - IEC2018029\n",
        "2. Rakamal Gupta - IEC2018050\n",
        "3. Rayan Kejriwal - IEC2018080\n",
        "4. Muasim Wani - IEC2018085\n",
        "5. Pavan Kalyan - IEC2018088\n",
        "\n",
        "under Prof. Muneendra Ojha, \n",
        "Dept. of IT, IIIT Allahabad, Prayagraj, India,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ5Rw95LEHSM"
      },
      "source": [
        "> `Objective : \"To Train a GPT model on a dedicated addition dataset to see if a Transformer can learn to add.\"`\n",
        "\n",
        "Our Objective is inspired by the addition section in the GPT-3 paper (Language Models are a few shot learners)- https://arxiv.org/pdf/2005.14165v4.pdf\n",
        "\n",
        "Code References Used :\n",
        "1. https://github.com/openai/gpt-2 - This has GPT code (only model code is taken but not training) in Tensorflow, which is converted to our usecase in PyTorch here.\n",
        "2. Image GPT by OpenAI - https://github.com/openai/image-gpt \n",
        "3. \"Attention is all you need paper\" - https://arxiv.org/pdf/1706.03762.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXM1sLNI1x0v"
      },
      "source": [
        "`> Note: We advice to enable GPU before running this notebook on GoogleColab.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BBh6SucOrTW"
      },
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX27-hv_GrkK"
      },
      "source": [
        "# Imports\n",
        "import math\n",
        "import logging\n",
        "import gc\n",
        "import os\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data.dataloader import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwC71udQPnT8"
      },
      "source": [
        "## 2. Setting Our Seed "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2GY0RxFLVAt"
      },
      "source": [
        "# Seeding \n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Making deterministic, setting our seed\n",
        "set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbunfaORMXmZ"
      },
      "source": [
        "## 3. Generating Our Datasets:\n",
        "\n",
        "In order to generate training and validation data, we define our custom Addition Dataset Class. </br>\n",
        "</n>\n",
        "The sum of two n-digit numbers gives a third up to (n+1)-digit number. So our\n",
        "encoding will simply be the n-digit first number, n-digit second number, \n",
        "and (n+1)-digit result, all simply concatenated together. Because each addition\n",
        "problem is so structured, there is no need to bother the model with encoding\n",
        "+, =, or other tokens. Each possible sequence has the same length, and simply\n",
        "contains the raw digits of the addition problem.\n",
        "\n",
        "As a few examples, the 2-digit problems:\n",
        "- 85 + 50 = 135 becomes the sequence [8, 5, 5, 0, 1, 3, 5]\n",
        "- 6 + 39 = 45 becomes the sequence [0, 6, 3, 9, 0, 4, 5]\n",
        "etc.\n",
        "\n",
        "We will also only train GPT on the final (n+1)-digits because the first\n",
        "two n-digits are always assumed to be given. So when we give GPT an exam later,\n",
        "we will e.g. feed it the sequence [0, 6, 3, 9], which encodes that we'd like\n",
        "to add 6 + 39, and hope that the model completes the integer sequence with [0, 4, 5] in 3 sequential steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFqYoAHsNxGi"
      },
      "source": [
        "#collapse-hide\n",
        "from torch.utils.data import Dataset\n",
        "class AdditionDataset(Dataset):\n",
        "  \n",
        "    \"\"\"\n",
        "    Our Custom Dataset Class for Generating Data into Training and Test sets.\n",
        "    Returns addition problems of up to some number of digits in the inputs. We recall\n",
        "    that all GPT cares about are sequences of integers, and completing them according to\n",
        "    patterns in the data. Therefore, we have to somehow encode addition problems\n",
        "    as a sequence of integers.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ndigit, split):\n",
        "        self.split = split # train/test\n",
        "        self.ndigit = ndigit\n",
        "        self.vocab_size = 10 # 10 possible digits 0..9\n",
        "        #+1 due to potential carry overflow, but then -1 because very last digit doesn't plug back\n",
        "        self.block_size = ndigit + ndigit + ndigit + 1 - 1\n",
        "        \n",
        "        #split up all addition problems into either training data or test data : \n",
        "        num = (10**self.ndigit)**2 # total number of possible combinations, here num = 10000\n",
        "        r = np.random.RandomState(1337) # making our datasets deterministic\n",
        "        perm = r.permutation(num) #perm is an array of indexes\n",
        "        num_test = min(int(num*0.2), 1000)# 20% of the whole dataset, or only up to 1000\n",
        "        self.ixes = perm[:num_test] if split == 'test' else perm[num_test:] # Here, We have taken 1000 examples in test set and 9000 in training set\n",
        "\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.ixes.size # Magic method for using len(...)\n",
        "\n",
        "\n",
        "    # Defining Magic Method __getitem__ for to use Dataset Class object as an iterable container.\n",
        "    def __getitem__(self, idx):\n",
        "        # given a problem index idx, first recover the associated a + b\n",
        "        idx = self.ixes[idx]\n",
        "        nd = 10**self.ndigit\n",
        "        a = idx // nd\n",
        "        b = idx %  nd\n",
        "        c = a + b\n",
        "        render = f'%0{self.ndigit}d%0{self.ndigit}d%0{self.ndigit+1}d' % (a,b,c) # e.g. 03+25=28 becomes \"0325028\" \n",
        "        dix = [int(s) for s in render] # convert each character to its token index\n",
        "        # x will be input to GPT and y will be the associated expected outputs\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long) # predict the next token in the sequence\n",
        "        y[:self.ndigit*2-1] = -100 # we will only train in the output locations. -100 will mask loss to zero\n",
        "        return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCL1jCkmphsT"
      },
      "source": [
        "Creating our Training and Test Datasets for 2-Digit Addition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVU9dr90h7H-"
      },
      "source": [
        "ndigit = 2\n",
        "train_dataset = AdditionDataset(ndigit=ndigit, split='train')\n",
        "test_dataset = AdditionDataset(ndigit=ndigit, split='test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbE7ebAk9bP-"
      },
      "source": [
        "Sample a training instance just to see what one raw example looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ksbx6ci9drE",
        "outputId": "ca6615f7-4204-4cba-8f79-fda5748c3103"
      },
      "source": [
        "train_dataset[0] "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([4, 7, 1, 7, 0, 6]), tensor([-100, -100, -100,    0,    6,    4]))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjgHDAo59gUe"
      },
      "source": [
        "## 4. Defining our GPT Model\n",
        "\n",
        "- The initial stem consists of a combination of token encoding and a positional encoding\n",
        "- The Core of our model is a uniform sequence of Transformer blocks :\n",
        "     * each Transformer is a sequential combination of a 1-hidden-layer MLP block and a self-attention block\n",
        "     * all blocks feed into a central residual pathway similar to resnets\n",
        "\n",
        "- The final decoder is a linear projection into a vanilla Softmax classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcTHraMR9geH"
      },
      "source": [
        "### 4.1 Our basic config classes for GPT MODEL \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPqyrS7QAaNG"
      },
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class GPTConfig:\n",
        "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
        "    embd_pdrop = 0.1\n",
        "    resid_pdrop = 0.1\n",
        "    attn_pdrop = 0.1\n",
        "\n",
        "    def __init__(self, vocab_size, block_size, **kwargs):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        for k,v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "class GPT1Config(GPTConfig):\n",
        "    \"\"\" GPT-1 like network roughly 125M params \"\"\"\n",
        "    n_layer = 12\n",
        "    n_head = 12\n",
        "    n_embd = 768"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFzzn8vdBh4A"
      },
      "source": [
        "### 4.2 Implementing Self-Attention Class from Scratch \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnFZpfnAW20C"
      },
      "source": [
        "![MHA](https://ars.els-cdn.com/content/image/1-s2.0-S0167639320302806-gr2.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXrsKQDuS9aA"
      },
      "source": [
        "Now, We will write our own class of Masked-MuliHead-Self Attention Block from scratch. Multi Head attention is perhaps one of the most important module of the transformer architecture. In case of transformers - they use a mechanism called self-attention instead of simple attention. Difference between simple attention and self-attention is that  - simple attention selectively focuses more on words which are present in query but in self-attention relationship with other surrounding(less-important) words is also taken into account to get a deep understanding of the context. In our model case of GPT , we have particularly used masked self attention which just means that, words to the right are no taken into account \n",
        "\n",
        "\n",
        "1. First our inputs of size (B,T,C) viz.( Mini-Batch Size, Embedding Size, ) is fed to the model. \n",
        "2. There are 3 Linear Layers to which our inputs are fed which then output Queries, Keys, and Values of dim (Inp_vector,T).\n",
        "3. Then we do (Queries@(keys).T)/sqrt(Embedding_Size) as our next step in order to calculate the attention score matrix.\n",
        "4. We then apply masking matrix to this matrix, to convert it to lower diagonal matrix for making our attention to the left words only in future.\n",
        "5. This matrix is then passed to softmax function which nornmalized all attention scores and also converts entries in the upper triangular half of -inf (in our case we have taken -100) to 0.\n",
        "6. We further do a dropout layer for regularization (with p_atten_drop = 0.1) and then finally we do a matmul with the original value matrix of dim -> (input_dim, embedding_size)\n",
        "7. We then project our output to the same dimension as that of input by passing it to a linear layer to again get an output of size(B,T,C) so that further we can concatenate it with the ouputs of the other heads along the outermost dimension.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRycbR_aBzH0"
      },
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    It is also possible to use torch.nn.MultiheadAttention here.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads\n",
        "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # regularization\n",
        "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
        "        # output projection\n",
        "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                     .view(1, 1, config.block_size, config.block_size))\n",
        "        self.n_head = config.n_head\n",
        "\n",
        "    def forward(self, x, layer_past=None):\n",
        "        B, T, C = x.size() #B->Batch Size, T->#Training rows, C->#Columns\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim where hs = C//self.n_head\n",
        "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_drop(att)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_drop(self.proj(y))\n",
        "        return y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AUuE-YkS8cx"
      },
      "source": [
        "### 4.3 Our Basic Transformer Block \n",
        "\n",
        "Here, we have defined a basic block which uses a config as an input (config is an instance of our GPTConfig Class) and defines the structure of a basic transformer block which will be used in future. \n",
        "\n",
        "- [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) is an linear layer normalization class inside Pytorch nn.Module used to apply Layer Normalization over a mini-batch of inputs.\n",
        "\n",
        "1. First we apply a layer norm to a batch of inputs, which we pass on to the masked self-attention block and then add the input x again to the residual of the self attention block in order to capture original information again after the self-attention block. -> x1\n",
        "\n",
        "2. After that, we again pass that to a Normalizatoin layer, which we pass to a multilayered feed forward network with Linear->Gelu->Linear->Drouput layers to which we add input x1 of the above step 1.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4tEG24OS7hb"
      },
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" our basic Transformer block \"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            nn.Dropout(config.resid_pdrop),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYPKHjkMgPPs"
      },
      "source": [
        "### 4.4 Full GPT Model Class:\n",
        "This is our main GPT model class for which we have defined our components as small blocks stated in various small classed explained above.\n",
        "\n",
        "1. This class also takes input config which is an instance of GPTConfig Class.\n",
        "2. The functionalyti of each function which we use is expalined within the function body itself. (Pls. refer there for details).\n",
        "3. For the forward function of this class, we take an input as the single row i.e one training row from our instance of AdditionDataset class.\n",
        "4.  #b=no. of examples in minibatch, t = #tokens in an example (maximum value of t = 6). our minibatch matrix of size (b,t).\n",
        "5. Our token embedding layer and pos_embedding layer are defined as :\n",
        "`> self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)` </br>\n",
        "`> self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))`\n",
        "Our inputs are passed to these layers and then outputs of these layers are added as together so as to capture the positional information (which is a must required information in case of transformars model as compared to RNN/LSTM models which are already sequential).\n",
        "\n",
        "6. Now we will just calculate and return the logits (probability function) of each of the digits in our vocab_size of 10 along with the loss (which we calculate only if the targets (y's) are provided initially). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxpiCKBaeWxc"
      },
      "source": [
        "class GPT(nn.Module):\n",
        "    \"\"\" This is our the full GPT language model, with a context size of block_size \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # input embedding stem\n",
        "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
        "        self.drop = nn.Dropout(config.embd_pdrop)\n",
        "        # transformer      \n",
        "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "        # decoder head\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        self.block_size = config.block_size\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.block_size\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "\n",
        "  \n",
        "    def configure_optimizers(self, train_config):\n",
        "\n",
        "\n",
        "      \"\"\"\n",
        "      By this function, We are separating out all parameters of the model into two buckets: \n",
        "      those that will experience weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
        "      We are then returning the PyTorch optimizer object.\n",
        "\n",
        "      \"\"\"\n",
        "      # separate out all parameters to those that will and won't experience regularizing weight decay\n",
        "      decay = set()\n",
        "      no_decay = set()\n",
        "      whitelist_weight_modules = (torch.nn.Linear, )\n",
        "      blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "      for mn, m in self.named_modules():\n",
        "          for pn, p in m.named_parameters():\n",
        "              fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
        "\n",
        "              if pn.endswith('bias'):\n",
        "                  # all biases will not be decayed\n",
        "                  no_decay.add(fpn)\n",
        "              elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "                  # weights of whitelist modules will be weight decayed\n",
        "                  decay.add(fpn)\n",
        "              elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                  # weights of blacklist modules will NOT be weight decayed\n",
        "                  no_decay.add(fpn)\n",
        "\n",
        "      # special case the position embedding parameter in the root GPT module as not decayed\n",
        "      no_decay.add('pos_emb')\n",
        "\n",
        "      # validate that we considered every parameter\n",
        "      param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "      inter_params = decay & no_decay\n",
        "      union_params = decay | no_decay\n",
        "      assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
        "      assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
        "                                                % (str(param_dict.keys() - union_params), )\n",
        "\n",
        "      # create the pytorch optimizer object\n",
        "      optim_groups = [\n",
        "          {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
        "          {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "      ]\n",
        "      optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
        "      return optimizer\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
        "\n",
        "        # forward the GPT model\n",
        "        token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
        "        position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
        "        x = self.drop(token_embeddings + position_embeddings)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        # if we are given some desired targets also calculate the loss\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return logits, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXFo5xsMpQua"
      },
      "source": [
        "### We can now initialize our GPT Model with assumable parameters :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0H11nTGpYqN",
        "outputId": "fa22ff09-0077-4295-840c-d91f0c69787c"
      },
      "source": [
        "#hide\n",
        "print(\"Our Vocab Size: \",train_dataset.vocab_size,\" Our Max. Block Size: \" ,train_dataset.block_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our Vocab Size:  10  Our Max. Block Size:  6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQNft4tQp6B_"
      },
      "source": [
        "# initialize a GPT model\n",
        "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, n_layer=2, n_head=4, n_embd=128)\n",
        "model = GPT(mconf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-cQ3RicqHQG"
      },
      "source": [
        "##  5. Trainer (Learner) for our GPT Model : \n",
        "Now we will define the trainer class of our model, to which we will  pass our defined instance of GPTModel along with some other tunable hyperparameters which are used in training in PyTorch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU_59o4RrIJh"
      },
      "source": [
        "### 5.1 Trainer Config Class\n",
        "Just like our GPT1Config and GPTCOnfig classes defined above, we have defined a seperate class for the main Traning class. It contains the hyperparameters which are used globally throughout in the main Trainer Class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA0Ac5jnqP1h"
      },
      "source": [
        "#collapse-hide \n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class TrainerConfig:\n",
        "    # optimization parameters\n",
        "    max_epochs = 10\n",
        "    batch_size = 64\n",
        "    learning_rate = 3e-4\n",
        "    betas = (0.9, 0.95)\n",
        "    grad_norm_clip = 1.0\n",
        "    weight_decay = 0.1 # only applied on matmul weights\n",
        "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
        "    lr_decay = False\n",
        "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
        "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
        "    # checkpoint settings\n",
        "    ckpt_path = None\n",
        "    num_workers = 0 # for DataLoader\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k,v in kwargs.items():\n",
        "            setattr(self, k, v)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnkvGpRlroyz"
      },
      "source": [
        "### 5.2 Main Trainer/Learner Class\n",
        "This is the main class used for our GPT Model Training, it contains the instances of GPT model , train_dataset, test_dataset, config classes as inputs.\n",
        "\n",
        "The model training goes on this way :\n",
        "1. First we construct 2 seperate dataloaders from PyTorch Dataloader class by using train_dataset, test_dataset which basically offer the functionality to load the data in size of minibatches of (x,y) on to the cpu/gpu whichever device is available.\n",
        "2. then we collect our logits,losses from the output of out GPT() model.\n",
        "3. We then use PyTorch's autograd mechanasim in order to backprop and update the parameters.\n",
        "*   `model.zero_grad()` - sets gradients to zero so that they don't accumulate.\n",
        "*   `loss.backward()` -  does the backpropogation step.\n",
        "*   `optimizer.step()` - updates the parameters throughout our model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MBP0sicq7re"
      },
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, train_dataset, test_dataset, config):\n",
        "        self.model = model\n",
        "        self.train_dataset = train_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "        self.config = config\n",
        "\n",
        "        # take over whatever gpus are on the system\n",
        "        self.device = 'cpu'\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.cuda.current_device()\n",
        "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        # DataParallel wrappers keep raw model object in .module attribute\n",
        "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
        "        logger.info(\"saving %s\", self.config.ckpt_path)\n",
        "        torch.save(raw_model.state_dict(), self.config.ckpt_path)\n",
        "\n",
        "    def train(self):\n",
        "        model, config = self.model, self.config\n",
        "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
        "        optimizer = raw_model.configure_optimizers(config)\n",
        "\n",
        "        def run_epoch(split):\n",
        "            is_train = split == 'train'\n",
        "            model.train(is_train)\n",
        "            data = self.train_dataset if is_train else self.test_dataset\n",
        "            loader = DataLoader(data, shuffle=True, pin_memory=True,\n",
        "                                batch_size=config.batch_size,\n",
        "                                num_workers=config.num_workers)\n",
        "\n",
        "            losses = []\n",
        "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
        "            for it, (x, y) in pbar:\n",
        "\n",
        "                # place data on the correct device\n",
        "                x = x.to(self.device)\n",
        "                y = y.to(self.device)\n",
        "\n",
        "                # forward the model\n",
        "                with torch.set_grad_enabled(is_train):\n",
        "                    logits, loss = model(x, y)\n",
        "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
        "                    losses.append(loss.item())\n",
        "\n",
        "                if is_train:\n",
        "                    # backprop and update the parameters\n",
        "                    model.zero_grad()\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
        "                    optimizer.step()\n",
        "\n",
        "                    # decay the learning rate based on our progress\n",
        "                    if config.lr_decay:\n",
        "                        self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
        "                        if self.tokens < config.warmup_tokens:\n",
        "                            # linear warmup\n",
        "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
        "                        else:\n",
        "                            # cosine learning rate decay\n",
        "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
        "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "                        lr = config.learning_rate * lr_mult\n",
        "                        for param_group in optimizer.param_groups:\n",
        "                            param_group['lr'] = lr\n",
        "                    else:\n",
        "                        lr = config.learning_rate\n",
        "\n",
        "                    # report progress\n",
        "                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. lr {lr:e}\")\n",
        "\n",
        "            if not is_train:\n",
        "                test_loss = float(np.mean(losses))\n",
        "                logger.info(\"test loss: %f\", test_loss)\n",
        "                return test_loss\n",
        "\n",
        "        best_loss = float('inf')\n",
        "        self.tokens = 0 # counter used for learning rate decay\n",
        "        for epoch in range(config.max_epochs):\n",
        "\n",
        "            run_epoch('train')\n",
        "            if self.test_dataset is not None:\n",
        "                test_loss = run_epoch('test')\n",
        "\n",
        "            # supports early stopping based on the test loss, or just save always if no test set is provided\n",
        "            good_model = self.test_dataset is None or test_loss < best_loss\n",
        "            if self.config.ckpt_path is not None and good_model:\n",
        "                best_loss = test_loss\n",
        "                self.save_checkpoint()\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSLsg5e8uP96"
      },
      "source": [
        "## 6. Model Training :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEuYDl1sugWB",
        "outputId": "a4cc013d-7741-44d3-d390-b3899f4c1e76"
      },
      "source": [
        "# initialize a trainer instance and kick off training\n",
        "from tqdm import tqdm\n",
        "tconf = TrainerConfig(max_epochs=50, batch_size=512, learning_rate=6e-4,\n",
        "                      lr_decay=True, warmup_tokens=1024, final_tokens=50*len(train_dataset)*(ndigit+1),\n",
        "                      num_workers=4)\n",
        "trainer = Trainer(model, train_dataset, test_dataset, tconf)\n",
        "trainer.train()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "epoch 1 iter 17: train loss 1.74271. lr 5.994512e-04: 100%|██████████| 18/18 [00:01<00:00, 14.37it/s]\n",
            "epoch 2 iter 17: train loss 1.51097. lr 5.977197e-04: 100%|██████████| 18/18 [00:01<00:00, 17.21it/s]\n",
            "epoch 3 iter 17: train loss 1.32211. lr 5.948114e-04: 100%|██████████| 18/18 [00:01<00:00, 17.80it/s]\n",
            "epoch 4 iter 17: train loss 1.19657. lr 5.907379e-04: 100%|██████████| 18/18 [00:00<00:00, 18.51it/s]\n",
            "epoch 5 iter 17: train loss 1.14752. lr 5.855153e-04: 100%|██████████| 18/18 [00:01<00:00, 17.75it/s]\n",
            "epoch 6 iter 17: train loss 1.10465. lr 5.791641e-04: 100%|██████████| 18/18 [00:00<00:00, 18.21it/s]\n",
            "epoch 7 iter 17: train loss 1.08063. lr 5.717095e-04: 100%|██████████| 18/18 [00:00<00:00, 18.92it/s]\n",
            "epoch 8 iter 17: train loss 1.04661. lr 5.631810e-04: 100%|██████████| 18/18 [00:00<00:00, 19.82it/s]\n",
            "epoch 9 iter 17: train loss 0.94335. lr 5.536122e-04: 100%|██████████| 18/18 [00:00<00:00, 19.41it/s]\n",
            "epoch 10 iter 17: train loss 0.61353. lr 5.430411e-04: 100%|██████████| 18/18 [00:00<00:00, 19.91it/s]\n",
            "epoch 11 iter 17: train loss 0.52056. lr 5.315093e-04: 100%|██████████| 18/18 [00:00<00:00, 19.92it/s]\n",
            "epoch 12 iter 17: train loss 0.45946. lr 5.190624e-04: 100%|██████████| 18/18 [00:00<00:00, 19.11it/s]\n",
            "epoch 13 iter 17: train loss 0.41773. lr 5.057497e-04: 100%|██████████| 18/18 [00:00<00:00, 19.13it/s]\n",
            "epoch 14 iter 17: train loss 0.34299. lr 4.916238e-04: 100%|██████████| 18/18 [00:00<00:00, 19.04it/s]\n",
            "epoch 15 iter 17: train loss 0.30794. lr 4.767405e-04: 100%|██████████| 18/18 [00:00<00:00, 18.23it/s]\n",
            "epoch 16 iter 17: train loss 0.29624. lr 4.611586e-04: 100%|██████████| 18/18 [00:00<00:00, 18.71it/s]\n",
            "epoch 17 iter 17: train loss 0.26151. lr 4.449397e-04: 100%|██████████| 18/18 [00:00<00:00, 18.74it/s]\n",
            "epoch 18 iter 17: train loss 0.22487. lr 4.281479e-04: 100%|██████████| 18/18 [00:00<00:00, 18.48it/s]\n",
            "epoch 19 iter 17: train loss 0.20200. lr 4.108497e-04: 100%|██████████| 18/18 [00:00<00:00, 18.69it/s]\n",
            "epoch 20 iter 17: train loss 0.16634. lr 3.931133e-04: 100%|██████████| 18/18 [00:00<00:00, 19.10it/s]\n",
            "epoch 21 iter 17: train loss 0.17252. lr 3.750088e-04: 100%|██████████| 18/18 [00:00<00:00, 18.21it/s]\n",
            "epoch 22 iter 17: train loss 0.15451. lr 3.566079e-04: 100%|██████████| 18/18 [00:01<00:00, 17.97it/s]\n",
            "epoch 23 iter 17: train loss 0.14947. lr 3.379832e-04: 100%|██████████| 18/18 [00:00<00:00, 18.65it/s]\n",
            "epoch 24 iter 17: train loss 0.12629. lr 3.192084e-04: 100%|██████████| 18/18 [00:01<00:00, 17.56it/s]\n",
            "epoch 25 iter 17: train loss 0.11772. lr 3.003577e-04: 100%|██████████| 18/18 [00:01<00:00, 17.39it/s]\n",
            "epoch 26 iter 17: train loss 0.11381. lr 2.815056e-04: 100%|██████████| 18/18 [00:00<00:00, 18.44it/s]\n",
            "epoch 27 iter 17: train loss 0.16899. lr 2.627266e-04: 100%|██████████| 18/18 [00:01<00:00, 17.43it/s]\n",
            "epoch 28 iter 17: train loss 0.10231. lr 2.440948e-04: 100%|██████████| 18/18 [00:00<00:00, 18.49it/s]\n",
            "epoch 29 iter 17: train loss 0.08752. lr 2.256841e-04: 100%|██████████| 18/18 [00:00<00:00, 18.43it/s]\n",
            "epoch 30 iter 17: train loss 0.09948. lr 2.075671e-04: 100%|██████████| 18/18 [00:00<00:00, 18.13it/s]\n",
            "epoch 31 iter 17: train loss 0.09623. lr 1.898155e-04: 100%|██████████| 18/18 [00:00<00:00, 18.16it/s]\n",
            "epoch 32 iter 17: train loss 0.07943. lr 1.724993e-04: 100%|██████████| 18/18 [00:01<00:00, 17.79it/s]\n",
            "epoch 33 iter 17: train loss 0.06771. lr 1.556871e-04: 100%|██████████| 18/18 [00:01<00:00, 17.57it/s]\n",
            "epoch 34 iter 17: train loss 0.06776. lr 1.394453e-04: 100%|██████████| 18/18 [00:00<00:00, 18.95it/s]\n",
            "epoch 35 iter 17: train loss 0.07907. lr 1.238381e-04: 100%|██████████| 18/18 [00:00<00:00, 19.35it/s]\n",
            "epoch 36 iter 17: train loss 0.07799. lr 1.089272e-04: 100%|██████████| 18/18 [00:00<00:00, 19.50it/s]\n",
            "epoch 37 iter 17: train loss 0.08679. lr 9.477150e-05: 100%|██████████| 18/18 [00:00<00:00, 19.37it/s]\n",
            "epoch 38 iter 17: train loss 0.06168. lr 8.142699e-05: 100%|██████████| 18/18 [00:00<00:00, 18.58it/s]\n",
            "epoch 39 iter 17: train loss 0.06297. lr 6.894639e-05: 100%|██████████| 18/18 [00:00<00:00, 18.86it/s]\n",
            "epoch 40 iter 17: train loss 0.06865. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 18.81it/s]\n",
            "epoch 41 iter 17: train loss 0.07373. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 19.54it/s]\n",
            "epoch 42 iter 17: train loss 0.06541. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 19.05it/s]\n",
            "epoch 43 iter 17: train loss 0.06758. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 19.30it/s]\n",
            "epoch 44 iter 17: train loss 0.08270. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 19.62it/s]\n",
            "epoch 45 iter 17: train loss 0.08476. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 19.03it/s]\n",
            "epoch 46 iter 17: train loss 0.07492. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 18.81it/s]\n",
            "epoch 47 iter 17: train loss 0.06154. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 18.13it/s]\n",
            "epoch 48 iter 17: train loss 0.08624. lr 6.000000e-05: 100%|██████████| 18/18 [00:01<00:00, 17.21it/s]\n",
            "epoch 49 iter 17: train loss 0.07017. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 18.23it/s]\n",
            "epoch 50 iter 17: train loss 0.04833. lr 6.000000e-05: 100%|██████████| 18/18 [00:01<00:00, 17.67it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMr8BI9buh1p"
      },
      "source": [
        "## 7. Getting Accuracy on Training and Validation DataSets :\n",
        "\n",
        "Now we will evaluate our miniature version of the transformer GPT model trained on our custom dataset (9000->training set size), (1000->validation set size) by providing it with an exam of doing Addition. Here, we also define our basic utilitiy functions which are used for sampling and doing inference on our training and validatoin sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twOjPtKa0-2B"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMAWax--04UE"
      },
      "source": [
        "#collapse-hide\n",
        "# Taking Top-k Logits  \n",
        "def top_k_logits(logits, k):\n",
        "    v, ix = torch.topk(logits, k)\n",
        "    out = logits.clone()\n",
        "    out[out < v[:, [-1]]] = -float('Inf')\n",
        "    return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None):\n",
        "    \"\"\"\n",
        "    This function takes a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
        "    the sequence, feeding the predictions back into the model each time. \n",
        "\n",
        "    \"\"\"\n",
        "    block_size = model.get_block_size()\n",
        "    model.eval()\n",
        "    for k in range(steps):\n",
        "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
        "        logits, _ = model(x_cond)\n",
        "        # pluck the logits at the final step and scale by temperature\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        # optionally crop probabilities to only the top k options\n",
        "        if top_k is not None:\n",
        "            logits = top_k_logits(logits, top_k)\n",
        "        # apply softmax to convert to probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # sample from the distribution or take the most likely\n",
        "        if sample:\n",
        "            ix = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "        # append to the sequence and continue\n",
        "        x = torch.cat((x, ix), dim=1)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1-cZK7-vVMP"
      },
      "source": [
        "#collapse-hide\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "def give_exam(dataset, batch_size=32, max_batches=-1):    \n",
        "    results = []\n",
        "    loader = DataLoader(dataset, batch_size=batch_size)\n",
        "    for b, (x, y) in enumerate(loader):\n",
        "        x = x.to(trainer.device)\n",
        "        d1d2 = x[:, :ndigit*2]\n",
        "        d1d2d3 = sample(model, d1d2, ndigit+1)\n",
        "        d3 = d1d2d3[:, -(ndigit+1):]\n",
        "        factors = torch.tensor([[10**i for i in range(ndigit+1)][::-1]]).to(trainer.device)\n",
        "        # decode the integers from individual digits\n",
        "        d1i = (d1d2[:,:ndigit] * factors[:,1:]).sum(1)\n",
        "        d2i = (d1d2[:,ndigit:ndigit*2] * factors[:,1:]).sum(1)\n",
        "        d3i_pred = (d3 * factors).sum(1)\n",
        "        d3i_gt = d1i + d2i\n",
        "        correct = (d3i_pred == d3i_gt).cpu() # Software 1.0 vs. Software 2.0 fight RIGHT on this line, lol\n",
        "        for i in range(x.size(0)):\n",
        "            results.append(int(correct[i]))\n",
        "            judge = 'YEP!!!' if correct[i] else 'NOPE'\n",
        "            if not correct[i]:\n",
        "                print(\"GPT claims that %03d + %03d = %03d (gt is %03d; %s)\" \n",
        "                      % (d1i[i], d2i[i], d3i_pred[i], d3i_gt[i], judge))\n",
        "        \n",
        "        if max_batches >= 0 and b+1 >= max_batches:\n",
        "            break\n",
        "\n",
        "    print(\"final score: %d/%d = %.2f%% correct\" % (np.sum(results), len(results), 100*np.mean(results)))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tbIVPZDVnNKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "563d7729-97e4-4591-eb25-a372e88cedf6"
      },
      "source": [
        "# training set: how well did we memorize?\n",
        "give_exam(train_dataset, batch_size=1024, max_batches=10)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT claims that 045 + 055 = 090 (gt is 100; NOPE)\n",
            "final score: 8999/9000 = 99.99% correct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4pyYVP-nNKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfa4d567-6ff1-4174-a3f8-e3c56c92e6a5"
      },
      "source": [
        "# test set: how well did we generalize?\n",
        "give_exam(test_dataset, batch_size=1024, max_batches=-1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT claims that 055 + 045 = 090 (gt is 100; NOPE)\n",
            "final score: 999/1000 = 99.90% correct\n"
          ]
        }
      ]
    }
  ]
}