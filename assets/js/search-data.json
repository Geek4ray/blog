{
  
    
        "post0": {
            "title": "Minimal implementation of GPT",
            "content": "This is Group-23 NLP Project : A Minimal Implemantation of GPT Model . Group Members : . Mallikarjuna Naik - IEC2018029 | Rakamal Gupta - IEC2018050 | Rayan Kejriwal - IEC2018080 | Muasim Wani - IEC2018085 | Pavan Kalyan - IEC2018088 | under Prof. Muneendra Ojha, Dept. of IT, IIIT Allahabad, Prayagraj, India, . Objective :&quot;To Train a GPT model on a dedicated addition dataset to see if a Transformer can learn to add.&quot; Our Objective is inspired by the addition section in the GPT-3 paper (Language Models are a few shot learners)- https://arxiv.org/pdf/2005.14165v4.pdf . Code References Used : . https://github.com/openai/gpt-2 - This has GPT code (only model code is taken but not training) in Tensorflow, which is converted to our usecase in PyTorch here. | Image GPT by OpenAI - https://github.com/openai/image-gpt | &quot;Attention is all you need paper&quot; - https://arxiv.org/pdf/1706.03762.pdf | &gt; Note: We advice to enable GPU before running this notebook on GoogleColab. . 1. Imports . import math import logging import gc import os import numpy as np import torchvision import torch import matplotlib.pyplot as plt import random import torch.nn as nn from torch.nn import functional as F import torch.optim as optim from torch.optim.lr_scheduler import LambdaLR from torch.utils.data.dataloader import DataLoader . 2. Setting Our Seed . def set_seed(seed): random.seed(seed) np.random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed_all(seed) # Making deterministic, setting our seed set_seed(42) . 3. Generating Our Datasets: . In order to generate training and validation data, we define our custom Addition Dataset Class. &lt;/br&gt; &lt;/n&gt; The sum of two n-digit numbers gives a third up to (n+1)-digit number. So our encoding will simply be the n-digit first number, n-digit second number, and (n+1)-digit result, all simply concatenated together. Because each addition problem is so structured, there is no need to bother the model with encoding +, =, or other tokens. Each possible sequence has the same length, and simply contains the raw digits of the addition problem. . As a few examples, the 2-digit problems: . 85 + 50 = 135 becomes the sequence [8, 5, 5, 0, 1, 3, 5] | 6 + 39 = 45 becomes the sequence [0, 6, 3, 9, 0, 4, 5] etc. | . We will also only train GPT on the final (n+1)-digits because the first two n-digits are always assumed to be given. So when we give GPT an exam later, we will e.g. feed it the sequence [0, 6, 3, 9], which encodes that we&#39;d like to add 6 + 39, and hope that the model completes the integer sequence with [0, 4, 5] in 3 sequential steps. . from torch.utils.data import Dataset class AdditionDataset(Dataset): &quot;&quot;&quot; Our Custom Dataset Class for Generating Data into Training and Test sets. Returns addition problems of up to some number of digits in the inputs. We recall that all GPT cares about are sequences of integers, and completing them according to patterns in the data. Therefore, we have to somehow encode addition problems as a sequence of integers. &quot;&quot;&quot; def __init__(self, ndigit, split): self.split = split # train/test self.ndigit = ndigit self.vocab_size = 10 # 10 possible digits 0..9 #+1 due to potential carry overflow, but then -1 because very last digit doesn&#39;t plug back self.block_size = ndigit + ndigit + ndigit + 1 - 1 #split up all addition problems into either training data or test data : num = (10**self.ndigit)**2 # total number of possible combinations, here num = 10000 r = np.random.RandomState(1337) # making our datasets deterministic perm = r.permutation(num) #perm is an array of indexes num_test = min(int(num*0.2), 1000)# 20% of the whole dataset, or only up to 1000 self.ixes = perm[:num_test] if split == &#39;test&#39; else perm[num_test:] # Here, We have taken 1000 examples in test set and 9000 in training set def __len__(self): return self.ixes.size # Magic method for using len(...) # Defining Magic Method __getitem__ for to use Dataset Class object as an iterable container. def __getitem__(self, idx): # given a problem index idx, first recover the associated a + b idx = self.ixes[idx] nd = 10**self.ndigit a = idx // nd b = idx % nd c = a + b render = f&#39;%0{self.ndigit}d%0{self.ndigit}d%0{self.ndigit+1}d&#39; % (a,b,c) # e.g. 03+25=28 becomes &quot;0325028&quot; dix = [int(s) for s in render] # convert each character to its token index # x will be input to GPT and y will be the associated expected outputs x = torch.tensor(dix[:-1], dtype=torch.long) y = torch.tensor(dix[1:], dtype=torch.long) # predict the next token in the sequence y[:self.ndigit*2-1] = -100 # we will only train in the output locations. -100 will mask loss to zero return x, y . . Creating our Training and Test Datasets for 2-Digit Addition . ndigit = 2 train_dataset = AdditionDataset(ndigit=ndigit, split=&#39;train&#39;) test_dataset = AdditionDataset(ndigit=ndigit, split=&#39;test&#39;) . Sample a training instance just to see what one raw example looks like . train_dataset[0] . (tensor([4, 7, 1, 7, 0, 6]), tensor([-100, -100, -100, 0, 6, 4])) . 4. Defining our GPT Model . The initial stem consists of a combination of token encoding and a positional encoding | The Core of our model is a uniform sequence of Transformer blocks : . each Transformer is a sequential combination of a 1-hidden-layer MLP block and a self-attention block | all blocks feed into a central residual pathway similar to resnets | . | The final decoder is a linear projection into a vanilla Softmax classifier . | . 4.1 Our basic config classes for GPT MODEL . logger = logging.getLogger(__name__) class GPTConfig: &quot;&quot;&quot; base GPT config, params common to all GPT versions &quot;&quot;&quot; embd_pdrop = 0.1 resid_pdrop = 0.1 attn_pdrop = 0.1 def __init__(self, vocab_size, block_size, **kwargs): self.vocab_size = vocab_size self.block_size = block_size for k,v in kwargs.items(): setattr(self, k, v) class GPT1Config(GPTConfig): &quot;&quot;&quot; GPT-1 like network roughly 125M params &quot;&quot;&quot; n_layer = 12 n_head = 12 n_embd = 768 . 4.2 Implementing Self-Attention Class from Scratch . . Now, We will write our own class of Masked-MuliHead-Self Attention Block from scratch. Multi Head attention is perhaps one of the most important module of the transformer architecture. In case of transformers - they use a mechanism called self-attention instead of simple attention. Difference between simple attention and self-attention is that - simple attention selectively focuses more on words which are present in query but in self-attention relationship with other surrounding(less-important) words is also taken into account to get a deep understanding of the context. In our model case of GPT , we have particularly used masked self attention which just means that, words to the right are no taken into account . First our inputs of size (B,T,C) viz.( Mini-Batch Size, Embedding Size, ) is fed to the model. | There are 3 Linear Layers to which our inputs are fed which then output Queries, Keys, and Values of dim (Inp_vector,T). | Then we do (Queries@(keys).T)/sqrt(Embedding_Size) as our next step in order to calculate the attention score matrix. | We then apply masking matrix to this matrix, to convert it to lower diagonal matrix for making our attention to the left words only in future. | This matrix is then passed to softmax function which nornmalized all attention scores and also converts entries in the upper triangular half of -inf (in our case we have taken -100) to 0. | We further do a dropout layer for regularization (with p_atten_drop = 0.1) and then finally we do a matmul with the original value matrix of dim -&gt; (input_dim, embedding_size) | We then project our output to the same dimension as that of input by passing it to a linear layer to again get an output of size(B,T,C) so that further we can concatenate it with the ouputs of the other heads along the outermost dimension. | class CausalSelfAttention(nn.Module): &quot;&quot;&quot; A vanilla multi-head masked self-attention layer with a projection at the end. It is also possible to use torch.nn.MultiheadAttention here. &quot;&quot;&quot; def __init__(self, config): super().__init__() assert config.n_embd % config.n_head == 0 # key, query, value projections for all heads self.key = nn.Linear(config.n_embd, config.n_embd) self.query = nn.Linear(config.n_embd, config.n_embd) self.value = nn.Linear(config.n_embd, config.n_embd) # regularization self.attn_drop = nn.Dropout(config.attn_pdrop) self.resid_drop = nn.Dropout(config.resid_pdrop) # output projection self.proj = nn.Linear(config.n_embd, config.n_embd) # causal mask to ensure that attention is only applied to the left in the input sequence self.register_buffer(&quot;mask&quot;, torch.tril(torch.ones(config.block_size, config.block_size)) .view(1, 1, config.block_size, config.block_size)) self.n_head = config.n_head def forward(self, x, layer_past=None): B, T, C = x.size() #B-&gt;Batch Size, T-&gt;#Training rows, C-&gt;#Columns # calculate query, key, values for all heads in batch and move head forward to be the batch dim where hs = C//self.n_head k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs) q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs) v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs) # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T) att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float(&#39;-inf&#39;)) att = F.softmax(att, dim=-1) att = self.attn_drop(att) y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs) y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side # output projection y = self.resid_drop(self.proj(y)) return y . 4.3 Our Basic Transformer Block . Here, we have defined a basic block which uses a config as an input (config is an instance of our GPTConfig Class) and defines the structure of a basic transformer block which will be used in future. . LayerNorm is an linear layer normalization class inside Pytorch nn.Module used to apply Layer Normalization over a mini-batch of inputs. | . First we apply a layer norm to a batch of inputs, which we pass on to the masked self-attention block and then add the input x again to the residual of the self attention block in order to capture original information again after the self-attention block. -&gt; x1 . | After that, we again pass that to a Normalizatoin layer, which we pass to a multilayered feed forward network with Linear-&gt;Gelu-&gt;Linear-&gt;Drouput layers to which we add input x1 of the above step 1. . | class Block(nn.Module): &quot;&quot;&quot; our basic Transformer block &quot;&quot;&quot; def __init__(self, config): super().__init__() self.ln1 = nn.LayerNorm(config.n_embd) self.ln2 = nn.LayerNorm(config.n_embd) self.attn = CausalSelfAttention(config) self.mlp = nn.Sequential( nn.Linear(config.n_embd, 4 * config.n_embd), nn.GELU(), nn.Linear(4 * config.n_embd, config.n_embd), nn.Dropout(config.resid_pdrop), ) def forward(self, x): x = x + self.attn(self.ln1(x)) x = x + self.mlp(self.ln2(x)) return x . 4.4 Full GPT Model Class: . This is our main GPT model class for which we have defined our components as small blocks stated in various small classed explained above. . This class also takes input config which is an instance of GPTConfig Class. | The functionalyti of each function which we use is expalined within the function body itself. (Pls. refer there for details). | For the forward function of this class, we take an input as the single row i.e one training row from our instance of AdditionDataset class. | #b=no. of examples in minibatch, t = #tokens in an example (maximum value of t = 6). our minibatch matrix of size (b,t). | Our token embedding layer and pos_embedding layer are defined as : &gt; self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd) &lt;/br&gt; &gt; self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd)) Our inputs are passed to these layers and then outputs of these layers are added as together so as to capture the positional information (which is a must required information in case of transformars model as compared to RNN/LSTM models which are already sequential). . | Now we will just calculate and return the logits (probability function) of each of the digits in our vocab_size of 10 along with the loss (which we calculate only if the targets (y&#39;s) are provided initially). . | class GPT(nn.Module): &quot;&quot;&quot; This is our the full GPT language model, with a context size of block_size &quot;&quot;&quot; def __init__(self, config): super().__init__() # input embedding stem self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd) self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd)) self.drop = nn.Dropout(config.embd_pdrop) # transformer self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)]) # decoder head self.ln_f = nn.LayerNorm(config.n_embd) self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False) self.block_size = config.block_size self.apply(self._init_weights) logger.info(&quot;number of parameters: %e&quot;, sum(p.numel() for p in self.parameters())) def get_block_size(self): return self.block_size def _init_weights(self, module): if isinstance(module, (nn.Linear, nn.Embedding)): module.weight.data.normal_(mean=0.0, std=0.02) if isinstance(module, nn.Linear) and module.bias is not None: module.bias.data.zero_() elif isinstance(module, nn.LayerNorm): module.bias.data.zero_() module.weight.data.fill_(1.0) def configure_optimizers(self, train_config): &quot;&quot;&quot; By this function, We are separating out all parameters of the model into two buckets: those that will experience weight decay for regularization and those that won&#39;t (biases, and layernorm/embedding weights). We are then returning the PyTorch optimizer object. &quot;&quot;&quot; # separate out all parameters to those that will and won&#39;t experience regularizing weight decay decay = set() no_decay = set() whitelist_weight_modules = (torch.nn.Linear, ) blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding) for mn, m in self.named_modules(): for pn, p in m.named_parameters(): fpn = &#39;%s.%s&#39; % (mn, pn) if mn else pn # full param name if pn.endswith(&#39;bias&#39;): # all biases will not be decayed no_decay.add(fpn) elif pn.endswith(&#39;weight&#39;) and isinstance(m, whitelist_weight_modules): # weights of whitelist modules will be weight decayed decay.add(fpn) elif pn.endswith(&#39;weight&#39;) and isinstance(m, blacklist_weight_modules): # weights of blacklist modules will NOT be weight decayed no_decay.add(fpn) # special case the position embedding parameter in the root GPT module as not decayed no_decay.add(&#39;pos_emb&#39;) # validate that we considered every parameter param_dict = {pn: p for pn, p in self.named_parameters()} inter_params = decay &amp; no_decay union_params = decay | no_decay assert len(inter_params) == 0, &quot;parameters %s made it into both decay/no_decay sets!&quot; % (str(inter_params), ) assert len(param_dict.keys() - union_params) == 0, &quot;parameters %s were not separated into either decay/no_decay set!&quot; % (str(param_dict.keys() - union_params), ) # create the pytorch optimizer object optim_groups = [ {&quot;params&quot;: [param_dict[pn] for pn in sorted(list(decay))], &quot;weight_decay&quot;: train_config.weight_decay}, {&quot;params&quot;: [param_dict[pn] for pn in sorted(list(no_decay))], &quot;weight_decay&quot;: 0.0}, ] optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas) return optimizer def forward(self, idx, targets=None): b, t = idx.size() assert t &lt;= self.block_size, &quot;Cannot forward, model block size is exhausted.&quot; # forward the GPT model token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector x = self.drop(token_embeddings + position_embeddings) x = self.blocks(x) x = self.ln_f(x) logits = self.head(x) # if we are given some desired targets also calculate the loss loss = None if targets is not None: loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1)) return logits, loss . We can now initialize our GPT Model with assumable parameters : . mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, n_layer=2, n_head=4, n_embd=128) model = GPT(mconf) . 5. Trainer (Learner) for our GPT Model : . Now we will define the trainer class of our model, to which we will pass our defined instance of GPTModel along with some other tunable hyperparameters which are used in training in PyTorch. . 5.1 Trainer Config Class . Just like our GPT1Config and GPTCOnfig classes defined above, we have defined a seperate class for the main Traning class. It contains the hyperparameters which are used globally throughout in the main Trainer Class. . logger = logging.getLogger(__name__) class TrainerConfig: # optimization parameters max_epochs = 10 batch_size = 64 learning_rate = 3e-4 betas = (0.9, 0.95) grad_norm_clip = 1.0 weight_decay = 0.1 # only applied on matmul weights # learning rate decay params: linear warmup followed by cosine decay to 10% of original lr_decay = False warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere final_tokens = 260e9 # (at what point we reach 10% of original LR) # checkpoint settings ckpt_path = None num_workers = 0 # for DataLoader def __init__(self, **kwargs): for k,v in kwargs.items(): setattr(self, k, v) . . 5.2 Main Trainer/Learner Class . This is the main class used for our GPT Model Training, it contains the instances of GPT model , train_dataset, test_dataset, config classes as inputs. . The model training goes on this way : . First we construct 2 seperate dataloaders from PyTorch Dataloader class by using train_dataset, test_dataset which basically offer the functionality to load the data in size of minibatches of (x,y) on to the cpu/gpu whichever device is available. | then we collect our logits,losses from the output of out GPT() model. | We then use PyTorch&#39;s autograd mechanasim in order to backprop and update the parameters. | model.zero_grad() - sets gradients to zero so that they don&#39;t accumulate. | loss.backward() - does the backpropogation step. | optimizer.step() - updates the parameters throughout our model. | . class Trainer: def __init__(self, model, train_dataset, test_dataset, config): self.model = model self.train_dataset = train_dataset self.test_dataset = test_dataset self.config = config # take over whatever gpus are on the system self.device = &#39;cpu&#39; if torch.cuda.is_available(): self.device = torch.cuda.current_device() self.model = torch.nn.DataParallel(self.model).to(self.device) def save_checkpoint(self): # DataParallel wrappers keep raw model object in .module attribute raw_model = self.model.module if hasattr(self.model, &quot;module&quot;) else self.model logger.info(&quot;saving %s&quot;, self.config.ckpt_path) torch.save(raw_model.state_dict(), self.config.ckpt_path) def train(self): model, config = self.model, self.config raw_model = model.module if hasattr(self.model, &quot;module&quot;) else model optimizer = raw_model.configure_optimizers(config) def run_epoch(split): is_train = split == &#39;train&#39; model.train(is_train) data = self.train_dataset if is_train else self.test_dataset loader = DataLoader(data, shuffle=True, pin_memory=True, batch_size=config.batch_size, num_workers=config.num_workers) losses = [] pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader) for it, (x, y) in pbar: # place data on the correct device x = x.to(self.device) y = y.to(self.device) # forward the model with torch.set_grad_enabled(is_train): logits, loss = model(x, y) loss = loss.mean() # collapse all losses if they are scattered on multiple gpus losses.append(loss.item()) if is_train: # backprop and update the parameters model.zero_grad() loss.backward() torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip) optimizer.step() # decay the learning rate based on our progress if config.lr_decay: self.tokens += (y &gt;= 0).sum() # number of tokens processed this step (i.e. label is not -100) if self.tokens &lt; config.warmup_tokens: # linear warmup lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens)) else: # cosine learning rate decay progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens)) lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress))) lr = config.learning_rate * lr_mult for param_group in optimizer.param_groups: param_group[&#39;lr&#39;] = lr else: lr = config.learning_rate # report progress pbar.set_description(f&quot;epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. lr {lr:e}&quot;) if not is_train: test_loss = float(np.mean(losses)) logger.info(&quot;test loss: %f&quot;, test_loss) return test_loss best_loss = float(&#39;inf&#39;) self.tokens = 0 # counter used for learning rate decay for epoch in range(config.max_epochs): run_epoch(&#39;train&#39;) if self.test_dataset is not None: test_loss = run_epoch(&#39;test&#39;) # supports early stopping based on the test loss, or just save always if no test set is provided good_model = self.test_dataset is None or test_loss &lt; best_loss if self.config.ckpt_path is not None and good_model: best_loss = test_loss self.save_checkpoint() . 6. Model Training : . from tqdm import tqdm tconf = TrainerConfig(max_epochs=50, batch_size=512, learning_rate=6e-4, lr_decay=True, warmup_tokens=1024, final_tokens=50*len(train_dataset)*(ndigit+1), num_workers=4) trainer = Trainer(model, train_dataset, test_dataset, tconf) trainer.train() . /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) epoch 1 iter 17: train loss 1.74271. lr 5.994512e-04: 100%|██████████| 18/18 [00:01&lt;00:00, 14.37it/s] epoch 2 iter 17: train loss 1.51097. lr 5.977197e-04: 100%|██████████| 18/18 [00:01&lt;00:00, 17.21it/s] epoch 3 iter 17: train loss 1.32211. lr 5.948114e-04: 100%|██████████| 18/18 [00:01&lt;00:00, 17.80it/s] epoch 4 iter 17: train loss 1.19657. lr 5.907379e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 18.51it/s] epoch 5 iter 17: train loss 1.14752. lr 5.855153e-04: 100%|██████████| 18/18 [00:01&lt;00:00, 17.75it/s] epoch 6 iter 17: train loss 1.10465. lr 5.791641e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 18.21it/s] epoch 7 iter 17: train loss 1.08063. lr 5.717095e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 18.92it/s] epoch 8 iter 17: train loss 1.04661. lr 5.631810e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 19.82it/s] epoch 9 iter 17: train loss 0.94335. lr 5.536122e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 19.41it/s] epoch 10 iter 17: train loss 0.61353. lr 5.430411e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 19.91it/s] epoch 11 iter 17: train loss 0.52056. lr 5.315093e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 19.92it/s] epoch 12 iter 17: train loss 0.45946. lr 5.190624e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 19.11it/s] epoch 13 iter 17: train loss 0.41773. lr 5.057497e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 19.13it/s] epoch 14 iter 17: train loss 0.34299. lr 4.916238e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 19.04it/s] epoch 15 iter 17: train loss 0.30794. lr 4.767405e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 18.23it/s] epoch 16 iter 17: train loss 0.29624. lr 4.611586e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 18.71it/s] epoch 17 iter 17: train loss 0.26151. lr 4.449397e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 18.74it/s] epoch 18 iter 17: train loss 0.22487. lr 4.281479e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 18.48it/s] epoch 19 iter 17: train loss 0.20200. lr 4.108497e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 18.69it/s] epoch 20 iter 17: train loss 0.16634. lr 3.931133e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 19.10it/s] epoch 21 iter 17: train loss 0.17252. lr 3.750088e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 18.21it/s] epoch 22 iter 17: train loss 0.15451. lr 3.566079e-04: 100%|██████████| 18/18 [00:01&lt;00:00, 17.97it/s] epoch 23 iter 17: train loss 0.14947. lr 3.379832e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 18.65it/s] epoch 24 iter 17: train loss 0.12629. lr 3.192084e-04: 100%|██████████| 18/18 [00:01&lt;00:00, 17.56it/s] epoch 25 iter 17: train loss 0.11772. lr 3.003577e-04: 100%|██████████| 18/18 [00:01&lt;00:00, 17.39it/s] epoch 26 iter 17: train loss 0.11381. lr 2.815056e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 18.44it/s] epoch 27 iter 17: train loss 0.16899. lr 2.627266e-04: 100%|██████████| 18/18 [00:01&lt;00:00, 17.43it/s] epoch 28 iter 17: train loss 0.10231. lr 2.440948e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 18.49it/s] epoch 29 iter 17: train loss 0.08752. lr 2.256841e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 18.43it/s] epoch 30 iter 17: train loss 0.09948. lr 2.075671e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 18.13it/s] epoch 31 iter 17: train loss 0.09623. lr 1.898155e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 18.16it/s] epoch 32 iter 17: train loss 0.07943. lr 1.724993e-04: 100%|██████████| 18/18 [00:01&lt;00:00, 17.79it/s] epoch 33 iter 17: train loss 0.06771. lr 1.556871e-04: 100%|██████████| 18/18 [00:01&lt;00:00, 17.57it/s] epoch 34 iter 17: train loss 0.06776. lr 1.394453e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 18.95it/s] epoch 35 iter 17: train loss 0.07907. lr 1.238381e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 19.35it/s] epoch 36 iter 17: train loss 0.07799. lr 1.089272e-04: 100%|██████████| 18/18 [00:00&lt;00:00, 19.50it/s] epoch 37 iter 17: train loss 0.08679. lr 9.477150e-05: 100%|██████████| 18/18 [00:00&lt;00:00, 19.37it/s] epoch 38 iter 17: train loss 0.06168. lr 8.142699e-05: 100%|██████████| 18/18 [00:00&lt;00:00, 18.58it/s] epoch 39 iter 17: train loss 0.06297. lr 6.894639e-05: 100%|██████████| 18/18 [00:00&lt;00:00, 18.86it/s] epoch 40 iter 17: train loss 0.06865. lr 6.000000e-05: 100%|██████████| 18/18 [00:00&lt;00:00, 18.81it/s] epoch 41 iter 17: train loss 0.07373. lr 6.000000e-05: 100%|██████████| 18/18 [00:00&lt;00:00, 19.54it/s] epoch 42 iter 17: train loss 0.06541. lr 6.000000e-05: 100%|██████████| 18/18 [00:00&lt;00:00, 19.05it/s] epoch 43 iter 17: train loss 0.06758. lr 6.000000e-05: 100%|██████████| 18/18 [00:00&lt;00:00, 19.30it/s] epoch 44 iter 17: train loss 0.08270. lr 6.000000e-05: 100%|██████████| 18/18 [00:00&lt;00:00, 19.62it/s] epoch 45 iter 17: train loss 0.08476. lr 6.000000e-05: 100%|██████████| 18/18 [00:00&lt;00:00, 19.03it/s] epoch 46 iter 17: train loss 0.07492. lr 6.000000e-05: 100%|██████████| 18/18 [00:00&lt;00:00, 18.81it/s] epoch 47 iter 17: train loss 0.06154. lr 6.000000e-05: 100%|██████████| 18/18 [00:00&lt;00:00, 18.13it/s] epoch 48 iter 17: train loss 0.08624. lr 6.000000e-05: 100%|██████████| 18/18 [00:01&lt;00:00, 17.21it/s] epoch 49 iter 17: train loss 0.07017. lr 6.000000e-05: 100%|██████████| 18/18 [00:00&lt;00:00, 18.23it/s] epoch 50 iter 17: train loss 0.04833. lr 6.000000e-05: 100%|██████████| 18/18 [00:01&lt;00:00, 17.67it/s] . 7. Getting Accuracy on Training and Validation DataSets : . Now we will evaluate our miniature version of the transformer GPT model trained on our custom dataset (9000-&gt;training set size), (1000-&gt;validation set size) by providing it with an exam of doing Addition. Here, we also define our basic utilitiy functions which are used for sampling and doing inference on our training and validatoin sets. . # Taking Top-k Logits def top_k_logits(logits, k): v, ix = torch.topk(logits, k) out = logits.clone() out[out &lt; v[:, [-1]]] = -float(&#39;Inf&#39;) return out @torch.no_grad() def sample(model, x, steps, temperature=1.0, sample=False, top_k=None): &quot;&quot;&quot; This function takes a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in the sequence, feeding the predictions back into the model each time. &quot;&quot;&quot; block_size = model.get_block_size() model.eval() for k in range(steps): x_cond = x if x.size(1) &lt;= block_size else x[:, -block_size:] # crop context if needed logits, _ = model(x_cond) # pluck the logits at the final step and scale by temperature logits = logits[:, -1, :] / temperature # optionally crop probabilities to only the top k options if top_k is not None: logits = top_k_logits(logits, top_k) # apply softmax to convert to probabilities probs = F.softmax(logits, dim=-1) # sample from the distribution or take the most likely if sample: ix = torch.multinomial(probs, num_samples=1) else: _, ix = torch.topk(probs, k=1, dim=-1) # append to the sequence and continue x = torch.cat((x, ix), dim=1) return x . . from torch.utils.data.dataloader import DataLoader def give_exam(dataset, batch_size=32, max_batches=-1): results = [] loader = DataLoader(dataset, batch_size=batch_size) for b, (x, y) in enumerate(loader): x = x.to(trainer.device) d1d2 = x[:, :ndigit*2] d1d2d3 = sample(model, d1d2, ndigit+1) d3 = d1d2d3[:, -(ndigit+1):] factors = torch.tensor([[10**i for i in range(ndigit+1)][::-1]]).to(trainer.device) # decode the integers from individual digits d1i = (d1d2[:,:ndigit] * factors[:,1:]).sum(1) d2i = (d1d2[:,ndigit:ndigit*2] * factors[:,1:]).sum(1) d3i_pred = (d3 * factors).sum(1) d3i_gt = d1i + d2i correct = (d3i_pred == d3i_gt).cpu() # Software 1.0 vs. Software 2.0 fight RIGHT on this line, lol for i in range(x.size(0)): results.append(int(correct[i])) judge = &#39;YEP!!!&#39; if correct[i] else &#39;NOPE&#39; if not correct[i]: print(&quot;GPT claims that %03d + %03d = %03d (gt is %03d; %s)&quot; % (d1i[i], d2i[i], d3i_pred[i], d3i_gt[i], judge)) if max_batches &gt;= 0 and b+1 &gt;= max_batches: break print(&quot;final score: %d/%d = %.2f%% correct&quot; % (np.sum(results), len(results), 100*np.mean(results))) . . give_exam(train_dataset, batch_size=1024, max_batches=10) . GPT claims that 045 + 055 = 090 (gt is 100; NOPE) final score: 8999/9000 = 99.99% correct . give_exam(test_dataset, batch_size=1024, max_batches=-1) . GPT claims that 055 + 045 = 090 (gt is 100; NOPE) final score: 999/1000 = 99.90% correct .",
            "url": "https://geek4ray.github.io/blog/mingpt/transformers/2021/11/22/Implementing-Miniature-GPT-Model.html",
            "relUrl": "/mingpt/transformers/2021/11/22/Implementing-Miniature-GPT-Model.html",
            "date": " • Nov 22, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Tesla AI Day",
            "content": ". Raw inputs which come into the stack and neural net processes into a vector space .",
            "url": "https://geek4ray.github.io/blog/fastpages/jupyter/2021/11/20/_09_19_Tesla_AI_Day.html",
            "relUrl": "/fastpages/jupyter/2021/11/20/_09_19_Tesla_AI_Day.html",
            "date": " • Nov 20, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "My Resume",
            "content": "Rayan Kejriwal . +91-94618 73955 | Bhiwadi, Rajasthan, India, 301019 | . aryankejriwal4@gmail.com | https://geek4ray.github.io/blog/ | linkedin.com/in/rayan-kejriwal/ | . **Data Science | Machine Learning | Quantitative Finance | Mathematics** | . Data Science Enthusiast actively following AI research community and seeking for automation in the real- world. Always willing to live in a competitive environment, meeting people and exploring new technologies leveraging the power of AI. . EDUCATION . B.Tech. Electronics and Communication 07/2018 – Present . Indian Institute of Information Technology, Allahabad CGPA - 8.19/10 . Senior Secondary 04/2017 – 04/2018 . Prince International School, Jaipur CBSE - 87.4% . Secondary 04/2015 – 04/2016 . Modern Public School, Bhiwadi CBSE – 10/10 . Technical Skills . Languages: C++, Python, SQL . | Frameworks: PyTorch, Fastai . | Tools/Software: Jupyter, VBA Excel, MATLAB . | Libraries: Numpy, Pandas, Scikit, Matplotlib, Seaborn . | . | Key Skills . Machine Learning | Neural Nets | EDA | Deep Reinforcement Learning | Numerical Methods | . Object-Oriented Programming | Probability | . Statistics | Quantitative Analysis | . Hypotheses Testing | Timeseries . | . SKILLS . PROJECTS . Numerai 07/2021 – Present . Open-Source Hedge Fund . Known as the hardest data science tournament on the planet . | Stock Data Modeling . | . Human Activity Recognition System 04/2021 – 05/2021 . B.Tech. Mini Project . Activity Classification based on Real-time Data from MPU-6050 sensor fitted on a Wearable Band . | Feature Engineered 140 Predictive Variables . | . Kaggle Competitions 12/2020 – Present . Coleridge Initiative . Entity Extraction using Transformers API. | . Jane Street Market Prediction . Binary Classification, Financial Timeseries Modeling using Neural Network. . | Processing with NVTabular + Rapids API along with Dask Framework including floating-point precision and Numba Speedup. | . Other Major Works 06/2020 – 04/2021 . Quant . Implemented Greeks (using FDM), Jump Diffusion Model, Implied Volatility Model, and European Vanilla option payoff using Black Scholes, Monte Carlo and Asian Discrete Path dependent formula. Currently Studying Financial applications using Reinforcement Learning. . | Used generic programming and functors in C++. . | Other supporting numerical method implementations include Decomposition methods – LU, Cholesky, QR and Thomas Algorithm using Eigen Library. . | . Multilabel Face Classification: CelebA Dataset . To characterize Celebrity faces with multi-label face attributes, used Fastai Data Block API, transfer learning using Resnet-50, and fine-tuning to achieve an accuracy of 92% . | Deployed the web app using Jupyter Widgets and Voila . | . Image Segmentation: Camvid Dataset . Pixel wise Segmentation of image using Dynamic U-NET, also implemented from scratch. . | Further improved the model by using novel approaches like image resizing, ranger optimizer, fit-flat cos, and Mish activation. Achieved state-of-the-art. . | . Price Prediction: Blue Book for Bulldozers . Predicting Auction Sale Prices of heavy automobile equipment used Random Forest also implemented Random forest Regressor from scratch. . | Model Interpretation using RF feature importance, Tree Interpreter, Partial Dependence Plot (waterfall), and Tree Variance. . | . Other Datasets Worked . Rossman, Oxford Pet, Biwi Head Pose, RedWine, Adult, MNIST, CIFAR, IMDB, House Prediction, and Titanic | . RESPONSIBILITIES . IIITA Info Communication and Incubation Centre 08/2018 – 06/2021 . Overall Coordinator . Invited Global AI Leaders from FAIR, Microsoft, IBM, NUS, U of T, and Harvard at E-Summit’21 conference sponsored by Walmart and Cisco, lead a team of 89 People; conducted 3 successful editions of E-Summit so far. (2018-2021) . | Lead Organizer of B-Hacks’20. (2020) . | Campus Director of Global Hult Prize, IIIT Allahabad Chapter for the first time. (2020) . | Visited 25+ Coworking spaces in Delhi-NCR to collaborate with start-ups sponsored by NewGen IEDC (2018) . | . HONORS . GATE Statistics: AIR-127 (2021) . | Dronathon 2019: Overall Winner (2019) . | 1st North India Open Karate Championship: Bronze (2012), Silver (2013) . | . INTERESTS . Martial Arts (Karate), Yoga and Meditation, Casual Music (Synth and Vocals). | .",
            "url": "https://geek4ray.github.io/blog/2021/11/20/My-Resume.html",
            "relUrl": "/2021/11/20/My-Resume.html",
            "date": " • Nov 20, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Introducing fastpages",
            "content": ". We are very pleased to announce the immediate availability of fastpages. fastpages is a platform which allows you to create and host a blog for free, with no ads and many useful features, such as: . Create posts containing code, outputs of code (which can be interactive), formatted text, etc directly from Jupyter Notebooks; for instance see this great example post from Scott Hawley. Notebook posts support features such as: Interactive visualizations made with Altair remain interactive. | Hide or show cell input and output. | Collapsable code cells that are either open or closed by default. | Define the Title, Summary and other metadata via a special markdown cells | Ability to add links to Colab and GitHub automatically. | . | Create posts, including formatting and images, directly from Microsoft Word documents. | Create and edit Markdown posts entirely online using GitHub&#39;s built-in markdown editor. | Embed Twitter cards and YouTube videos. | Categorization of blog posts by user-supplied tags for discoverability. | ... and much more | . fastpages relies on Github pages for hosting, and Github Actions to automate the creation of your blog. The setup takes around three minutes, and does not require any technical knowledge or expertise. Due to built-in automation of fastpages, you don&#39;t have to fuss with conversion scripts. All you have to do is save your Jupyter notebook, Word document or markdown file into a specified directory and the rest happens automatically. Infact, this blog post is written in a Jupyter notebook, which you can see with the &quot;View on GitHub&quot; link above. . fast.ai have previously released a similar project called fast_template, which is even easier to set up, but does not support automatic creation of posts from Microsoft Word or Jupyter notebooks, including many of the features outlined above. . Because fastpages is more flexible and extensible, we recommend using it where possible. fast_template may be a better option for getting folks blogging who have no technical expertise at all, and will only be creating posts using Github&#39;s integrated online editor. . Setting Up Fastpages . The setup process of fastpages is automated with GitHub Actions, too! Upon creating a repo from the fastpages template, a pull request will automatically be opened (after ~ 30 seconds) configuring your blog so it can start working. The automated pull request will greet you with instructions like this: . . All you have to do is follow these instructions (in the PR you receive) and your new blogging site will be up and running! . Jupyter Notebooks &amp; Fastpages . In this post, we will cover special features that fastpages provides for Jupyter notebooks. You can also write your blog posts with Word documents or markdown in fastpages, which contain many, but not all the same features. . Options via FrontMatter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . All of the above settings are enabled in this post, so you can see what they look like! . the summary field (preceeded by &gt;) will be displayed under your title, and will also be used by social media to display as the description of your page. | toc: setting this to true will automatically generate a table of contents | badges: setting this to true will display Google Colab and GitHub links on your blog post. | comments: setting this to true will enable comments. See these instructions for more details. | author this will display the authors names. | categories will allow your post to be categorized on a &quot;Tags&quot; page, where readers can browse your post by categories. | . Markdown front matter is formatted similarly to notebooks. The differences between the two can be viewed on the fastpages README. . Code Folding . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . If you want to completely hide cells (not just collapse them), read these instructions. . Interactive Charts With Altair . Interactive visualizations made with Altair remain interactive! . We leave this below cell unhidden so you can enjoy a preview of syntax highlighting in fastpages, which uses the Dracula theme. . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;IMDB_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget IMDB_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | 6.1 | . 1 First Love, Last Rites | 10876.0 | 300000.0 | 6.9 | . 2 I Married a Strange Person | 203134.0 | 250000.0 | 6.8 | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | NaN | . 4 Slam | 1087521.0 | 1000000.0 | 3.4 | . Other Features . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Images w/Captions . You can include markdown images with captions like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Of course, the caption is optional. . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . More Examples . This tutorial contains more examples of what you can do with notebooks. . How fastpages Converts Notebooks to Blog Posts . fastpages uses nbdev to power the conversion process of Jupyter Notebooks to blog posts. When you save a notebook into the /_notebooks folder of your repository, GitHub Actions applies nbdev against those notebooks automatically. The same process occurs when you save Word documents or markdown files into the _word or _posts directory, respectively. . We will discuss how GitHub Actions work in a follow up blog post. . Resources &amp; Next Steps . We highly encourage you to start blogging with fastpages! Some resources that may be helpful: . fastpages repo - this is where you can go to create your own fastpages blog! | Fastai forums - nbdev &amp; blogging category. You can ask questions about fastpages here, as well as suggest new features. | nbdev: this project powers the conversion of Jupyter notebooks to blog posts. | . If you end up writing a blog post using fastpages, please let us know on Twitter: @jeremyphoward, @HamelHusain. .",
            "url": "https://geek4ray.github.io/blog/fastpages/jupyter/2021/09/02/introducing-fastpages.html",
            "relUrl": "/fastpages/jupyter/2021/09/02/introducing-fastpages.html",
            "date": " • Sep 2, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Building Complex AI Algorithms from scratch",
            "content": "Lets talk about basic SGD in Pytorch . def f(x): return x**2 . xt = tensor(3.).requires_grad_() . Notice the special method requires_grad_? That&#39;s the magical incantation we use to tell PyTorch that we want to calculate gradients with respect to that variable at that value. It is essentially tagging the variable, so PyTorch will remember to keep track of how to compute gradients of the other, direct calculations on it that you will ask for. . yt = f(xt) . yt.backward() . The &quot;backward&quot; here refers to backpropagation, which is the name given to the process of calculating the derivative of each layer (in case of neural network). . xt.grad #(This grad arrtibute accumulates the gradient at the variable x and keeps adding them consecutively unless you zero out the recent grads by using **zero_grad** arrtibute. . tensor(6.) . xt.grad.zero_() . tensor(0.) . Now we can try with a vector : . def f(x): return (x**2).sum() . def calc_grad(x,f): y=f(x) y.backward() print(x.grad) x.grad.zero_() . calc_grad(tensor([3.,4.,5.],requires_grad=True),f) . tensor([ 6., 8., 10.]) . As we can see, we got right results! One thing is to notice that tensors always accept floating points and the grad can be implicitly created only for scalar outputs therefore the function f must return a scalar thats why we did apply .sum(). . Lets Take Another End-To-End Example of SGD . time = torch.arange(20).float() time . speed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1 plt.scatter(time,speed); . def f(t, params): a,b,c=params return a*(t**2) + b*t + c . params = torch.randn(3).requires_grad_() orig_params = params.clone() # Clone method will also copy autograd func. orig_params . tensor([0.2193, 0.4842, 0.5411], grad_fn=&lt;CloneBackward&gt;) . preds = f(time, params) . Let&#39;s create a little function to see how close our predictions are to our targets, and take a look: . def show_preds(preds, ax=None): if ax is None: ax=plt.subplots()[1] ax.scatter(time, speed) ax.scatter(time, (preds).detach().numpy(), color=&#39;red&#39;) ax.set_ylim(-300,100) . show_preds(preds) . loss = F.mse_loss(preds,speed) loss . tensor(1017.8171, grad_fn=&lt;MseLossBackward&gt;) . Our Goal is to now improve this loss. The next step is to calculate the gradients. . loss.backward() # .backward() method is applied on the variable of which we want to calculate gradient. params.grad # gradient is calculated w.r.t params -&gt;(a,b,c) . tensor([6582.9756, 437.4914, 13.7502]) . lr = 1e-5 params.grad * lr . tensor([0.0658, 0.0044, 0.0001]) . params.data -= params.grad.data*lr . params.grad = None . Understanding this bit depends on remembering recent history. To calculate the gradients we call backward on the loss. But this loss was itself calculated by mse, which in turn took preds as an input, which was calculated using f taking as an input params, which was the object on which we originally called requiredgrads—which is the original call that now allows us to call backward on loss. This chain of function calls represents the mathematical composition of functions, which enables PyTorch to use calculus&#39;s chain rule under the hood to calculate these gradients. . preds = f(time,params) F.mse_loss(preds,speed) . tensor(705.5072, grad_fn=&lt;MseLossBackward&gt;) . show_preds(preds) . def apply_step(params,prn=True): preds = f(time,params) loss = F.mse_loss(preds,speed) loss.backward() params.data -= params.grad.data*1e-5 params.grad = None if prn: print(loss.item()) #In this case our Metric is same as loss function return preds . for i in range(10): apply_step(params) . 646.4049072265625 642.8947143554688 634.5457153320312 632.962158203125 632.65869140625 632.5977172851562 632.5823974609375 632.5758666992188 632.5709228515625 632.5662841796875 . We just decided to stop after 10 epochs arbitrarily. In practice, we would watch the training and validation losses and our metrics to decide when to stop . params = torch.randn(3).requires_grad_() _,axs = plt.subplots(1,8,figsize=(24,3)) for ax in axs: show_preds(apply_step(params, False), ax) plt.tight_layout() . In 8 epochs, from left to right we can see that how our predictions are improving with a constant lr=1e-5 . 2) A simple Logistic Regression Class . class LogisticRegression(): def __init__(self) : pass def get_params(self,size) -&gt; torch.LongTensor: return torch.randn(size).requires_grad_() def sigmoid(self,x) : return 1/(1+torch.exp(-x)) def mse_loss(self,predictions,targets) -&gt; torch.LongTensor: return torch.where(targets==1, 1-predictions, predictions).mean() def fit(self,x,y,epochs,bs,lr,trim=False): dset = list(zip(x,y)) n_batches = int(len(dset)/bs) if (trim==False) : n_mini_batches= n_batches+int(len(dset)%bs) weights = self.get_params(x.shape[1]) bias = self.get_params(1) for e in range(epochs): for i in range(n_mini_batches): if(i == n_mini_batches-1) : xb = x[i*bs:] else : xb = x[i*bs:(i+1)*bs] preds = self.sigmoid(x@weights + bias) loss = self.mse_loss(preds,y) loss.backward() weights.data -= weights.grad.data*lr bias.data -= bias.grad.data*lr weights.grad = None bias.grad = None print(f&quot;Epoch_{e}_accuracy = &quot;,((preds&gt;=0.5)==y).float().mean().item()) def predict(self,x): return x*weights + bias . 3) Experimenting with MNIST : . (path/&#39;train&#39;).ls() . (#2) [Path(&#39;train/3&#39;),Path(&#39;train/7&#39;)] . (path/&#39;valid&#39;).ls() . (#2) [Path(&#39;valid/3&#39;),Path(&#39;valid/7&#39;)] . train_x = torch.cat([train_3_tens, train_7_tens]).view(-1, 28*28) valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28) train_y = tensor([1]*len(train_3_tens) + [0]*len(train_7_tens)).unsqueeze(1) valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1) . train_x.shape, valid_x.shape . (torch.Size([12396, 784]), torch.Size([2038, 784])) . m = LogisticRegression() . m.fit(x=train_x,y=train_y,bs=100,lr=1e-5,epochs=10) . Epoch_0_accuracy = 0.5040026903152466 Epoch_1_accuracy = 0.5040026903152466 Epoch_2_accuracy = 0.5040026903152466 Epoch_3_accuracy = 0.5040026903152466 Epoch_4_accuracy = 0.5040026903152466 Epoch_5_accuracy = 0.5040026903152466 Epoch_6_accuracy = 0.5040026903152466 Epoch_7_accuracy = 0.5040026903152466 Epoch_8_accuracy = 0.5040026903152466 Epoch_9_accuracy = 0.5040026903152466 . Putting it Altogether - Using Fastai Dataloaders ! . train_dset = list(zip(train_x,train_y)) valid_dset = list(zip(valid_x,valid_y)) . . Note: Now we will use fastai Dataloaders for to Load Data in mini-batches rathar than a single row in SGD. Just To say * Dataset - A list of tuples each tuple of form -&gt; $(x_i,y_i)$ . DataLoader - An Iterable/List of Minibatches where each minibatch is of the form $tuple(tensor(x_1,x_2,...,x_b), (y_1,y_2,...,y_b))$. | .",
            "url": "https://geek4ray.github.io/blog/ai/algorithms/2021/09/02/AI_Algorithms.html",
            "relUrl": "/ai/algorithms/2021/09/02/AI_Algorithms.html",
            "date": " • Sep 2, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My Name is Rayan Kejriwal, I am a currently a Final Year B.Tech. ECE undergraduate student at IIIT Allahabad, India. I am a Data Science Enthusiast and like to be a part of active AI research communities and seeking for automation in the real-world. I am always willing to live in a competitive environment, meeting people and exploring new technologies leveraging the power of AI. Also I am quite interested about Quantitative Finance and my current research interests include the applications of AI in Finance. . Contact me . aryankejriwal4@gmail.com .",
          "url": "https://geek4ray.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://geek4ray.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}