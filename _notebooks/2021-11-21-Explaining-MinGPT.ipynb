{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-11-21-Explaining MinGPT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN1iZ04nNRY6bv9clNZ7i1A"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ytIxWwZ2Q54"
      },
      "source": [
        "# Explaining MinGPT \n",
        "> This blog explains MinGPT Model implementation on Addition Data, by <a href=\"https://github.com/karpathy/minGPT\">Andrej Karpathy</a>.\n",
        "\n",
        "- toc: true \n",
        "- badges: true\n",
        "- comments: true\n",
        "- sticky_rank: 1\n",
        "- author: Rayan\n",
        "- image: images/diagram.png\n",
        "- categories: [MinGPT, transformers]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ5Rw95LEHSM"
      },
      "source": [
        "> `Objective : \"To Train a GPT model on a dedicated addition dataset to see if a Transformer can learn to add.\"`\n",
        "\n",
        "Our Objective is inspired by the addition section in the GPT-3 paper (Language Models are a few shot learners)- https://arxiv.org/pdf/2005.14165v4.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX27-hv_GrkK"
      },
      "source": [
        "#hide\n",
        "# Imports\n",
        "import math\n",
        "import logging\n",
        "import gc\n",
        "import os\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data.dataloader import DataLoader"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2GY0RxFLVAt"
      },
      "source": [
        "#collapse-hide\n",
        "#Seeding \n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# making deterministic, setting our seed\n",
        "set_seed(42)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbunfaORMXmZ"
      },
      "source": [
        "In order to generate data, we define our custom Addition Dataset Class. The sum of two n-digit numbers gives a third up to (n+1)-digit number. So our\n",
        "encoding will simply be the n-digit first number, n-digit second number, \n",
        "and (n+1)-digit result, all simply concatenated together. Because each addition\n",
        "problem is so structured, there is no need to bother the model with encoding\n",
        "+, =, or other tokens. Each possible sequence has the same length, and simply\n",
        "contains the raw digits of the addition problem.\n",
        "\n",
        "As a few examples, the 2-digit problems:\n",
        "- 85 + 50 = 135 becomes the sequence [8, 5, 5, 0, 1, 3, 5]\n",
        "- 6 + 39 = 45 becomes the sequence [0, 6, 3, 9, 0, 4, 5]\n",
        "etc.\n",
        "\n",
        "We will also only train GPT on the final (n+1)-digits because the first\n",
        "two n-digits are always assumed to be given. So when we give GPT an exam later,\n",
        "we will e.g. feed it the sequence [0, 6, 3, 9], which encodes that we'd like\n",
        "to add 6 + 39, and hope that the model completes the integer sequence with [0, 4, 5] in 3 sequential steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFqYoAHsNxGi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}